"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[249],{4133:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>n,metadata:()=>r,toc:()=>h});var o=i(4848),a=i(8453);const n={title:"Case Study",description:"Unilogs Case Study",toc_min_heading_level:2,toc_max_heading_level:3,className:"unilogs-case-study"},s="Case Study",r={type:"mdx",permalink:"/case-study",source:"@site/src/pages/case-study.md",title:"Case Study",description:"Unilogs Case Study",frontMatter:{title:"Case Study",description:"Unilogs Case Study",toc_min_heading_level:2,toc_max_heading_level:3,className:"unilogs-case-study"},unlisted:!1},l={},h=[{value:"Introduction",id:"introduction",level:2},{value:"Observability and Log Aggregation",id:"observability-and-log-aggregation",level:2},{value:"Existing Solutions",id:"existing-solutions",level:2},{value:"Unilogs Solution",id:"unilogs-solution",level:2},{value:"Unilogs Architecture",id:"unilogs-architecture",level:2},{value:"Unilogs Shipper - Vector",id:"unilogs-shipper---vector",level:3},{value:"Unilogs Observability Platform",id:"unilogs-observability-platform",level:3},{value:"Unilogs Entry Point - Kafka",id:"unilogs-entry-point---kafka",level:3},{value:"Unilogs Aggregation and Storage - Loki",id:"unilogs-aggregation-and-storage---loki",level:3},{value:"Unilogs Querying and Data Visualization - Grafana",id:"unilogs-querying-and-data-visualization---grafana",level:3},{value:"Design Strategy and Technical Challenges",id:"design-strategy-and-technical-challenges",level:2},{value:"The Challenge of Writing Unilogs&#39; Infrastructure as Code",id:"the-challenge-of-writing-unilogs-infrastructure-as-code",level:3},{value:"Reflections and Future Work",id:"reflections-and-future-work",level:2},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const t={h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.h1,{id:"case-study",children:"Case Study"}),"\n",(0,o.jsx)(t.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(t.p,{children:"Unilogs is an easy-to-deploy, reliable, and highly scalable log observability platform for distributed applications. It enables users to ship, transform, store, and visualize their logs without complex configuration using a self-hosted infrastructure."}),"\n",(0,o.jsx)(t.h2,{id:"observability-and-log-aggregation",children:"Observability and Log Aggregation"}),"\n",(0,o.jsx)(t.p,{children:"To understand what a log observability platform is, we first need to understand observability. Observability is, in brief, the ability to observe the inner workings of a software system. This is especially important in the context of a distributed application, where various functions are spread across different devices, making it difficult to get a holistic picture of its inner workings."}),"\n",(0,o.jsx)(t.p,{children:"For example, if a user experiences slow response times when ordering a pizza, there are several things that could be causing the delay."}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsx)(t.li,{children:"Network latency could be higher than normal"}),"\n",(0,o.jsx)(t.li,{children:"The validation of the order on the front-end application could be unusually slow"}),"\n",(0,o.jsx)(t.li,{children:"The backend server could be having difficulty connecting to the database"}),"\n",(0,o.jsx)(t.li,{children:"The database could be slow in finding and updating the \u201ccustomers\u201d or \u201corders\u201d table"}),"\n",(0,o.jsx)(t.li,{children:"The geolocation microservice responsible for generating an ETA could be slow"}),"\n",(0,o.jsx)(t.li,{children:"All of the above, but the delay is only noticeable or significant cumulatively"}),"\n"]}),"\n",(0,o.jsx)(t.p,{children:"Pinpointing the issue would be much more difficult without looking at information at the level of the entire application. The information generated by the app is called \u201ctelemetry\u201d, and there are 3 main types: metrics, traces, and logs. An \u201cobservability platform\u201d refers to an application which ingests, stores, and queries telemetry, and a \u201clog observability platform\u201d refers specifically to an observability platform that focuses on logs only."}),"\n",(0,o.jsx)(t.p,{children:"Each software system generates a large quantity of logs, a stream of detailed information occurring at each node of the system. This information is easy to under-utilize unless collected and shipped to a common observability platform which can aggregate, process, and store the log data. The data can then be analyzed to generate and display graphical reports. One such report might be of counts of the log level (i.e. of logs labeled as \u201cinfo,\u201d \u201cwarn,\u201d or \u201cerror\u201d), either by the total over time or by node/source. The user can drill down to the details of any particular part of that graph, even down to the level of the individual logs. In this way, the user trying to understand a complex bug can quickly identify where a spike in errors or warnings occurred, then view which sources were generating unusual amounts of those logs, and then examine the details in order to piece together the whole story."}),"\n",(0,o.jsxs)("figure",{className:"image-container",children:[(0,o.jsx)("img",{src:"/case-study/videos/log chaos.gif",className:"diagram",alt:"Log Observability Basics",width:"85%"}),(0,o.jsx)("figcaption",{align:"center",children:"Ocean of Logs"})]}),"\n",(0,o.jsx)(t.p,{children:"The alternative is to skim every individual log file for information that seems important, and attempt to mentally piece together some coherent meaning based on this heuristic sampling. Not only is this manual approach time consuming, it\u2019s also prone to human error. Even when done well, the data analyzed in this way is not collected and stored as a small part of an increasingly useful, central hub of information. Instead it is simply stored in the file system of each machine hosting any log-generating part of the distributed app, at least until such local storage becomes infeasible. As the scale and complexity of a distributed application increases, the need for a log observability platform grows alongside it."}),"\n",(0,o.jsx)(t.p,{children:"The challenge of setting up such a platform increases as well. Logs come in a variety of formats, requiring the log aggregation process to parse and transform them into a common structure that can be analyzed in bulk. Additionally, the large quantity of logs generated by an app means that any sizable distributed application could overwhelm a monolithic solution relying on a single machine to host the platform. In order to handle the scale of log aggregation in a larger production system, the log observability platform needs to have its functional component parts distributed across several machines (or virtual machines). For the purposes of this case study, a \u201ccomponent\u201d is defined as a logical part of the observability platform, separated by function and separately scalable according to how the increasing volume of the throughput impacts that part of the platform. When a \u201cshipper\u201d component is included, installed separately and independently from the observability platform, the shipper and platform can together be considered part of an \u201cobservability pipeline.\u201d"}),"\n",(0,o.jsxs)("figure",{className:"image-container",children:[(0,o.jsx)("img",{src:"/case-study/photos/Diagram 1.png",className:"diagram",alt:"Log Observability Basics",width:"85%"}),(0,o.jsx)("figcaption",{align:"center",children:"Basic Log Observability Pipeline"})]}),"\n",(0,o.jsx)(t.p,{children:"At a minimum, a log observability pipeline has 4 components:"}),"\n",(0,o.jsxs)(t.ol,{children:["\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"An agent aka shipper"})," - Collects the logs and ships them to the ingestor. The shipper can also parse and transform the logs before sending them on, ensuring that they end up in a common format."]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"An ingester"})," -  Receives the logs and inserts them into the storage component."]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Log storage component"})," - Some type of database which keeps the increasingly massive stores of logs in long-term storage."]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Data visualization component"})," - Queries and displays aggregate data about the logs\u2013preferably with the ability to quickly drill down to the details of any particular log source or time period."]}),"\n"]}),"\n",(0,o.jsx)(t.p,{children:"This design seems straightforward, but complications arise as the complexity of a distributed app increases, because each additional node in a distributed app is also another log source. As usage of the app increases, the rate of log generation for each source increases as well."}),"\n",(0,o.jsx)(t.p,{children:"A single ingester could quickly become a bottleneck on the system, dropping logs and losing data as the sheer volume overwhelms it. This points to horizontal scaling for that component, but that isn\u2019t simple, because log volume is bursty and unpredictable. Additionally, the storage itself quickly becomes a complex or expensive task\u2013how do you handle collecting TBs of data every day without maxing out your storage capacity or paying an unreasonable amount to store it with a cloud service? Do you compress that data somehow? How can you efficiently query it?"}),"\n",(0,o.jsx)(t.p,{children:"For these reasons and more, existing observability pipelines are generally more sophisticated and complex than the simple model depicted above."}),"\n",(0,o.jsx)(t.h2,{id:"existing-solutions",children:"Existing Solutions"}),"\n",(0,o.jsx)(t.p,{children:"Existing observability platforms come in two main categories\u2013managed services and self-hosted DIY setups. One kind of managed service is an end-to-end observability platform, like DataDog or New Relic. Another kind is a cloud-hosted, managed version of an otherwise open-source platform, like Grafana Cloud or Elastic Cloud, each of which is built using open-source components. Whether that\u2019s the Grafana Loki stack or the \u201cELK\u201d stack (Elasticsearch, Logstash, and Kibana), you pay for the cloud storage and for outsourcing the platform\u2019s maintenance. These underlying stacks are also two of the main options in the DIY, self-hosted category."}),"\n",(0,o.jsxs)("figure",{className:"image-container",children:[(0,o.jsx)("img",{src:"/case-study/photos/Diagram 2.png",className:"diagram",alt:"Existing Solutions Comparison",width:"85%"}),(0,o.jsx)("figcaption",{align:"center",children:"Comparison of Existing Log Observability Solutions"})]}),"\n",(0,o.jsx)(t.p,{children:"Each type of solution comes with its own advantages and disadvantages. The managed solutions are simpler to use, of course, but they also cost more. You\u2019re paying for both the cost of the infrastructure and the cost of the service managing the platform. You also have to give up ownership of your data to use the service, which may not be a viable option if your app produces logs with sensitive or even legally protected data (e.g. personal medical information)."}),"\n",(0,o.jsx)(t.p,{children:"On the other hand, going the DIY route can entail weeks of overcoming configuration quirks and steep learning curves. The ELK stack is notoriously difficult, but even the Grafana Loki stack is complex\u2013especially if you expand it to include additional components like a queue in front of the ingesters."}),"\n",(0,o.jsx)(t.p,{children:"Time spent upfront on a DIY stack is an investment, but it\u2019s also one which commits you to maintaining that complex system over time, both in terms of updating the infrastructure and  scaling appropriately. Even when scaling has been handled in advance through the use of a container orchestration system like Kubernetes, it comes with its own steep learning curve."}),"\n",(0,o.jsx)(t.p,{children:"Engineering time is money\u2013but it\u2019s also actually time, too. The need for an observability platform may become most obvious when there is already a problem for which insight is needed. But if an observability platform is being set up in response to a problem, spending a lot of time devising and building a DIY system would be prohibitively slow. Even if an observability pipeline is being built alongside a distributed application, that still slows down the work on what you actually care about\u2013the application itself. A new pipeline is also more likely to change shape in terms of its infrastructure than an established one\u2013which means any change to the log sources will entail another adjustment to the observability pipeline, possibly even cascading changes and/or configuration bugs."}),"\n",(0,o.jsx)(t.h2,{id:"unilogs-solution",children:"Unilogs Solution"}),"\n",(0,o.jsx)(t.p,{children:"Unilogs provides both the performance of a highly scalable log observability platform and the convenience of a streamlined setup. The Unilogs platform is production-ready, capable of automatically scaling to handle ingesting several TBs of logs per day. It has the resilience necessary to handle large spikes in throughput and the reliability to avoid losing any valid data through the stateful data persistence of the Kafka distributed streaming platform. It has the flexibility of a self-hosted solution, and the cost is limited to the cost of its infrastructure. Unilogs is easy to use, but it\u2019s not just a version of the naive log ingestion pipeline architecture described above\u2013it is a powerful, production-ready system."}),"\n",(0,o.jsxs)("figure",{className:"image-container",children:[(0,o.jsx)("img",{src:"/case-study/photos/Diagram 3.png",className:"diagram",alt:"Unilogs Solution Comparison",width:"85%"}),(0,o.jsx)("figcaption",{align:"center",children:"Unilogs Compared to Other Solutions"})]}),"\n",(0,o.jsx)(t.p,{children:"Unilogs combines key strengths of existing solutions: the ease of a managed service with the self-hosting and price point of a DIY solution. We focused on aggregating and analyzing logs, since they are the most ubiquitous type of telemetry. As an opinionated log observability platform, Unilogs comes pre-set with a particular infrastructure and a particular configuration. The Unilogs package comes with a set of Kafka message brokers in front, providing high reliability and resilience to the platform. Unilogs has a seamless setup, with a one-command deployment to AWS, the most popular cloud infrastructure provider."}),"\n",(0,o.jsx)(t.p,{children:"That said, Unilogs would mainly be used because the engineers relying on it don\u2019t already have the pre-existing expertise needed to quickly deploy a DIY stack. Our primary goal in building Unilogs was maximal simplicity for the user\u2013but we were careful to ensure that this was at the cost of flexibility only, not at the cost of scalability or reliability."}),"\n",(0,o.jsx)(t.p,{children:"The whole platform deploys with a single command and the provision of a few credentials. Instead of days or weeks, setup can be measured in minutes. There are only three prerequisites for deploying Unilogs:"}),"\n",(0,o.jsxs)(t.ol,{children:["\n",(0,o.jsx)(t.li,{children:"Node.js/npm installed on the machine that will do the deployment (generally already the case for a software engineer\u2019s machine)"}),"\n",(0,o.jsx)(t.li,{children:"Docker (and Node.js/npm) on any machine that will be setting up a log shipper"}),"\n",(0,o.jsx)(t.li,{children:"An AWS account with an IAM user with admin or otherwise sufficient permissions (which would need to be the case even going the DIY route, assuming you were still deploying to AWS)"}),"\n"]}),"\n",(0,o.jsxs)("figure",{className:"image-container",children:[(0,o.jsx)("img",{src:"/case-study/photos/Diagram 4.png",className:"diagram",alt:"Unilogs Deployment Simplicity",width:"85%"}),(0,o.jsx)("figcaption",{align:"center",children:"Simplified Deployment Process with Unilogs"})]}),"\n",(0,o.jsx)(t.p,{children:"That painful-looking list on the right is avoided because Unilogs\u2019 highly opinionated setup process enables us to pre-configure and automate away much of the complexity. This convenience comes at the cost of reduced flexibility, which manifests in a couple different ways."}),"\n",(0,o.jsx)(t.p,{children:"First, our solution deploys to AWS specifically. You cannot deploy Unilogs to another cloud computing provider, and you cannot deploy to your own machines on-site. Given that AWS is by far the most popular cloud infrastructure service, and the fact that self-hosting with on-site machines is unusual and in most cases comparatively undesirable, we felt that this was a reasonable price to pay for the convenience of a largely pre-configured system."}),"\n",(0,o.jsx)(t.p,{children:"Additionally, because Unilogs is a pre-packaged solution, it is also pre-built using specific components, and those components are configured to work together in a predetermined way. There is some limited flexibility, such as the option to use a different shipper than we provide, or the option to manually expand the configuration DIY-fashion to handle other types of telemetry beyond logs. But mostly, Unilogs does not have the flexibility of a fully DIY solution. As long as our setup used reliable, scalable architecture and reasonable default configurations, this seemed like a fair trade to make, since the users seeking a solution like Unilogs would likely not have the pre-existing expertise to have strong preferences on the choice of architecture and the configuration details."}),"\n",(0,o.jsx)(t.p,{children:"To really understand the advantages of the Unilogs architecture and how it can reliably handle such high throughput, we\u2019ll follow the journey of a log through the entire pipeline, from the log shipper to the Grafana UI."}),"\n",(0,o.jsx)(t.h2,{id:"unilogs-architecture",children:"Unilogs Architecture"}),"\n",(0,o.jsx)(t.p,{children:"The beginning of the pipeline is the Unilogs shipper, which ships logs from the client\u2019s host machines running parts of a distributed application. The main body of the pipeline is the Unilogs observability platform deployed on AWS, which ingests, processes, stores, and queries the log data. Finally, at the other end of the pipeline, Unilogs emits visualizations of the log data to the user\u2019s browser when they sign into the Grafana front end. This high level view can be seen in the diagram below."}),"\n",(0,o.jsxs)("figure",{className:"image-container",children:[(0,o.jsx)("img",{src:"/case-study/photos/Diagram 5.png",className:"diagram",alt:"Unilogs Architecture",width:"85%"}),(0,o.jsx)("figcaption",{align:"center",children:"Unilogs High-Level Architecture"})]}),"\n",(0,o.jsx)(t.h3,{id:"unilogs-shipper---vector",children:"Unilogs Shipper - Vector"}),"\n",(0,o.jsx)(t.p,{children:"Distributed app nodes typically output logs to files. From there, a log shipper (aka an \u201cagent\u201d) is installed on the same machine so it can tail those log files and ship any new log entries somewhere. In Unilogs\u2019 architecture, that log shipper is Vector. Vector is a newer, highly performant shipper which is very lightweight (only requires a few MB of memory)\u2013a choice we made to avoid putting too much burden on the client\u2019s host machines, since we don\u2019t know how much spare memory they have to add something on top of the distributed app nodes running on those machines. Vector also has native TCP connectivity to Kafka, making it a good fit for our pipeline (i.e. we didn\u2019t have to use a reverse proxy to first adapt HTTP messages)."}),"\n",(0,o.jsx)(t.p,{children:"Because this first step in the process occurs outside the deployed Unilogs platform, it was important to simplify the setup process for the shipper as well. For this reason, we built the Unilogs shipper configuration generator, a small node app that walks the user through the process of configuring and running a Dockerized container of Vector. The configuration generator runs on the command line, and simply asks for the user to point it to the path of the log file they want to ship, and to confirm the log format these logs are written in. If the log format is an unusual or custom format, Vector can still be DIY configured to parse those custom logs\u2013but for the most common formats, the user can simply select it using our tool. The configuration generator can then install and run the Vector container, and begin shipping to the Unilogs observability platform."}),"\n",(0,o.jsx)(t.p,{children:"See it in action below:"}),"\n",(0,o.jsxs)("figure",{className:"image-container",children:[(0,o.jsx)("img",{src:"/case-study/videos/unilogs-shipper.gif",className:"diagram",alt:"Log Observability Basics",width:"85%"}),(0,o.jsx)("figcaption",{align:"center",children:"Unilogs Shipper Demo"})]}),"\n",(0,o.jsx)(t.h3,{id:"unilogs-observability-platform",children:"Unilogs Observability Platform"}),"\n",(0,o.jsx)(t.p,{children:"The main part of the Unilogs log observability pipeline is the Unilogs platform deployed on AWS. This is the part of the pipeline that does all the work\u2013ingesting and aggregating the logs, processing them for storage, and querying and displaying log data visualizations. You can see in the diagram below that this is accomplished by several components working together to make a complete system, each of which is capable of scaling independently of the others as needed."}),"\n",(0,o.jsxs)("figure",{className:"image-container",children:[(0,o.jsx)("img",{src:"/case-study/photos/Diagram 6.png",className:"diagram",alt:"Unilogs Platform Details",width:"85%"}),(0,o.jsx)("figcaption",{align:"center",children:"Unilogs Platform Components"})]}),"\n",(0,o.jsx)(t.p,{children:"As shown above, the entire Unilogs platform is contained within a single Kubernetes cluster, hosted on AWS EKS. (Note: Grafana is included in the platform here because its backend is contained within the cluster, even though its front-end is client-rendered on the user\u2019s browser.) Kubernetes is a container orchestration engine, and using it allows Unilogs to set up rules during deployment that determine how to automatically scale all parts of the infrastructure as needed to meet demand. At the level of the Kubernetes pods\u2013networks of containerized functional components that make up Unilogs\u2013this scaling is done both in terms of the number of replicas (horizontal scaling) and the portion of compute power allotted from the available total to each replica (vertical scaling). Additionally, the Kubernetes cluster is configured to automatically scale its worker nodes (horizontally), i.e. change the total available compute power from AWS that runs the Unilogs platform."}),"\n",(0,o.jsx)(t.p,{children:"As we take a look at each major piece of Unilogs in turn, we\u2019ll be able to see the ways that the tasks, capacity, and needs of each part of the platform differ, and how they all coordinate together using Kubernetes to form a highly scalable and reliable observability pipeline."}),"\n",(0,o.jsx)(t.h3,{id:"unilogs-entry-point---kafka",children:"Unilogs Entry Point - Kafka"}),"\n",(0,o.jsx)(t.p,{children:"The first stop a log makes after being shipped to the Unilogs platform is Kafka, which serves as a queue in front of the rest of the log aggregation system. Placing a queue in front of a log aggregation system like Loki is a common way to resolve a common problem in log aggregation\u2013bursts of high volume."}),"\n",(0,o.jsx)(t.p,{children:"Pretty much every node of a distributed application will be generating frequent logs, and they all have to be stored somewhere. Depending on the size and type of the application, the volume of logs to ingest in total can be quite high, and that volume is increasing over time. A smaller business may measure daily log ingestion in GBs, but many larger businesses measure their daily log ingestion in TBs. Our choice of log aggregation system, Loki, is deployed in a mode that can handle several TBs of log data per day, which would be enough for many businesses with a moderately high log ingestion requirement."}),"\n",(0,o.jsx)(t.p,{children:"However, logs are a bursty type of data. They are generated in response to events, so when an app is malfunctioning and throwing cascading errors, the output could multiply many times compared to the standard pace of log generation when the app is functioning normally. This is also when logs become most useful, since you\u2019d want to be able to look into the details of what went wrong. So, even though our deployment of Loki can handle a large volume of logs, it\u2019s still possible that a big burst of logs could overwhelm its log ingestion components. Thus, the need for some sort of queue that would allow for asynchronous log processing that doesn\u2019t have to always match the pace of ingestion."}),"\n",(0,o.jsx)(t.p,{children:"Like many others designing log observability platforms, we chose Kafka to function as our queue, although it is more than that. Kafka is typically described as a distributed streaming platform and message broker. It is a heavyweight component, often chosen for its high throughput (about 5TB/day per broker) and relatively low latency."}),"\n",(0,o.jsx)(t.p,{children:"More than that, though, it is also a platform capable of storing and recovering large amounts of data, making use of data replication across brokers to ensure reliable delivery. Our Loki deployment may be able to scale automatically to handle large volumes, but large bursts of data could mean that a lot of logs get dropped due to Loki\u2019s preset rate limiting (needed to avoid overwhelming the system) just when that data is needed the most."}),"\n",(0,o.jsx)(t.p,{children:"The Unilogs platform deploys with three Kafka brokers, each of which manages its own partition of the total log data and replicates that data across the other brokers (for reliability). All in all, this means that Kafka enables Unilogs to sustain reliable data delivery even at peak loads."}),"\n",(0,o.jsx)(t.p,{children:"On the other end of the Kafka queue, we again employ Vector\u2013this time as a log \u201caggregator\u201d rather than as an \u201cagent\u201d, i.e. it processes and forwards the logs taken from potentially multiple Kafka brokers, rather than collecting them from the machine it\u2019s on. Vector workers are set to pull messages from any available Kafka broker and then process and forward them to Loki in the format it expects."}),"\n",(0,o.jsx)(t.h3,{id:"unilogs-aggregation-and-storage---loki",children:"Unilogs Aggregation and Storage - Loki"}),"\n",(0,o.jsx)(t.p,{children:"When Loki receives a log from a Vector worker, it does more than simply store it. Loki is actually a complex log aggregation system, with eight different sub-components which can be split out entirely and managed as a web of microservices (for maximal scaling at the cost of significant maintenance), or even just lumped together into a single monolithic app (for a simple setup that can only handle about 20GB of logs per day). Between those two extremes, we have \u201csimple scalable\u201d mode, which logically groups these sub-components by broader functions into three main \u201ctargets\u201d that translate into separately scaling groups of Kubernetes pods\u2013read, write, and backend."}),"\n",(0,o.jsxs)("figure",{className:"image-container",children:[(0,o.jsx)("img",{src:"/case-study/photos/Diagram 7.png",className:"diagram",alt:"Loki Architecture",width:"85%"}),(0,o.jsx)("figcaption",{align:"center",children:"Loki Simple Scalable Mode Architecture"})]}),"\n",(0,o.jsx)(t.p,{children:"Unilogs deploys Loki in \u201csimple scalable\u201d mode to optimize between scalability and simplicity. This still entails a significantly more complicated setup than the monolithic version, but we abstract that away from the user in order to yield the benefits without this downside."}),"\n",(0,o.jsx)(t.p,{children:'Part of the difference between the monolithic and the "simple scalable" deployment is that instead of simply storing the aggregate logs on its host machine\u2019s file system, we need a common storage location that can be accessed by all of the read, write, and backend targets which is able to handle the massive scale of storage necessary to hold onto the log data.'}),"\n",(0,o.jsx)(t.p,{children:"For that, Unilogs configures Loki to use two AWS S3 buckets (an object storage service), one for the logs themselves and another to store indices that speed up querying.\nLoki works together with the S3 buckets to store, index, compress, manage, and query logs, ultimately retrieving this data for the final Unilogs component, Grafana."}),"\n",(0,o.jsx)(t.h3,{id:"unilogs-querying-and-data-visualization---grafana",children:"Unilogs Querying and Data Visualization - Grafana"}),"\n",(0,o.jsx)(t.p,{children:"As just mentioned, Loki includes a sub-component (in its read targets) that queries the aggregated log data, but only when instructed to do so. The request for that data first originates from the data visualization component, which is responsible for handling requests from the user to display the retrieved log data in various ways. In the Unilogs architecture, this role is served by Grafana."}),"\n",(0,o.jsx)(t.p,{children:"The Unilogs platform deployment automatically generates an external IP which the user can use to access Grafana\u2019s UI. Once signed in with the user\u2019s chosen credentials, they can customize the dashboard, make custom queries into the log data provisioned from Loki, and receive graphical reports."}),"\n",(0,o.jsx)(t.p,{children:"The end result of the entire journey through the Unilogs pipeline, from the log file on the client\u2019s host machine to the data visualization that comes out the other end, is the ability to meaningfully examine aggregate log data as well as drill down to any specific source or log of interest to see the preserved details\u2013reliable log observability, even during peak bursts of activity."}),"\n",(0,o.jsx)(t.h2,{id:"design-strategy-and-technical-challenges",children:"Design Strategy and Technical Challenges"}),"\n",(0,o.jsx)(t.p,{children:"A repeated theme is the prioritization we placed on a simple user experience when deploying Unilogs, and the efforts we took to provide that while minimizing the trade-offs this decision entailed. The main method by which this was achieved was building Unilogs as a highly opinionated platform\u2013almost all configuration is preset, minimizing the headache of deployment but also the user\u2019s control over the details."}),"\n",(0,o.jsx)(t.p,{children:"Unilogs does not prevent power users from making some of their own decisions and modifying Unilogs through some DIY work, but there are degrees of feasibility for these changes. Some of the opinions wrapped up in the Unilogs package can be adjusted after deployment using the AWS console and/or command line tools (e.g. by changing the scaling min/max of the platform\u2019s underlying cloud compute or adding an AWS IAM role that other users can assume to get internal cluster access)."}),"\n",(0,o.jsx)(t.p,{children:"More difficult than post-hoc adjustments to deployment are some changes that would have to be made beforehand, such as by digging into our code and altering the configuration details for each component (Kafka, Vector, Loki, Grafana)."}),"\n",(0,o.jsx)(t.p,{children:"That said, we were less concerned about these more difficult adjustments, because our use case is users who don\u2019t have the time to (or aren\u2019t comfortable with) managing the configuration themselves, and who would benefit from those details being abstracted away by the Unilogs deployment package."}),"\n",(0,o.jsx)(t.p,{children:"This approach, however, entailed one major challenge for us\u2013in order for Unilogs users to do things the easy way, we as developers had to do things the hard way."}),"\n",(0,o.jsx)(t.h3,{id:"the-challenge-of-writing-unilogs-infrastructure-as-code",children:"The Challenge of Writing Unilogs' Infrastructure as Code"}),"\n",(0,o.jsx)(t.p,{children:"The AWS CDK is a way to write \u201cinfrastructure as code\u201d\u2013your code should specify what pieces of infrastructure will be deployed, how they are configured to work individually and in concert, what permissions and resources they each have, etc., and then you can run a couple commands in the terminal to execute that deployment based on the code in your files."}),"\n",(0,o.jsx)(t.p,{children:"This is easier said than done. The AWS CDK documentation is spotty, sometimes inaccurate, and occasionally out of date. \u201cHow-to\u201d guides on AWS generally give instructions using the AWS console and/or using AWS command line tools like eksctl and the AWS CLI (combined with other command line tools specific to Kubernetes, like helm and kubectl), and have no guidance on how to accomplish these various critical tasks using the CDK. Multiple times we found a recent feature that seemed like it would be a huge benefit, tried to implement it, and ultimately realized it was impossible using the CDK. One prime example is \u201cEKS Auto Mode\u201d, which promises to abstract away the complexity of managing an EKS cluster\u2019s security configuration, permissions, storage, and scaling details. This would have simplified much of our work, but the latest stable version of the CDK just didn\u2019t have any functionality relating to that feature. (Perhaps a future version of Unilogs could make use of it with the next version of the CDK.)"}),"\n",(0,o.jsx)(t.p,{children:"Or sometimes, even worse, the feature could be partially incorporated into the CDK, but in a way that subtly conflicts with other CDK functionality and breaks your code. This was the case with the recent \u201cPod Identity\u201d method of granting permissions to cluster addons, which is meant to streamline the old process. But because attempting to use the new authorization process created duplicate service accounts, breaking the whole deployment, we ended up reverting to the older, more complicated \u201cIAM Roles for Service Accounts\u201d (IRSA) method."}),"\n",(0,o.jsx)(t.p,{children:"Each of the command line tools we cut out of our process were created for a reason\u2013they reduce the number of steps it takes to do something, or even accomplish things the CDK simply can\u2019t. These tools abstract steps away from the user, often performing prerequisite tasks without the user even learning that there were any. And, of course, the use of these tools was better documented, with step-by-step guides available for each critical task we researched."}),"\n",(0,o.jsx)(t.p,{children:"So why did we choose to implement Unilogs deployment using only the CDK? The goal was to remove prerequisites from the process of deploying Unilogs (recall the diagram earlier comparing our deployment steps with those of a DIY process). With Unilogs, the user doesn\u2019t need to install Helm, kubectl, eksctl, or the AWS CLI, and they therefore also don\u2019t need to execute any extra manual commands using those tools or copy and paste a bunch of configuration details. All these steps would be eliminated as long as we could complete the process entirely using the CDK, which is what we ultimately managed to do."}),"\n",(0,o.jsx)(t.p,{children:"The AWS CDK is not a user-friendly node package. But, in the end, it is what allowed us to make Unilogs user-friendly."}),"\n",(0,o.jsx)(t.h2,{id:"reflections-and-future-work",children:"Reflections and Future Work"}),"\n",(0,o.jsx)(t.p,{children:"In keeping with the simplicity-first principle discussed above, for future work we would like to add support for more options for users, as long as that doesn\u2019t result in shifting more of the configuration burden onto them. This would cut down on the trade-off of a less flexible deployment, the main cost paid by prioritizing simplicity."}),"\n",(0,o.jsx)(t.p,{children:"For example, the Unilogs platform could be expanded to process metrics and traces, two other key pieces of telemetry that would build Unilogs into a more complete observability platform. As mentioned when discussing our architecture, we chose Vector as a shipper and Kafka consumer in part because it is capable of transmitting metrics and traces in addition to logs. It would take significantly more setup, and involve instrumenting the user\u2019s applications on their host machines, but it could be done without changing our core architecture."}),"\n",(0,o.jsx)(t.p,{children:"There are also smaller pieces of functionality that would be valuable to add in the future. As mentioned in the design decisions section above, users could alter the way permissions are granted to enable more users to access the Unilogs cluster through an IAM role, but this is not currently something we provide support for. However, we could provide options in the initial deployment process that would allow users to set this up from the start, either in addition to or instead of granting admin access to the deploying user."}),"\n",(0,o.jsx)(t.p,{children:"One more ambitious expansion we considered was enabling multi-cloud support using Terraform\u2013a cloud-agnostic provisioning tool which would allow us to write deployment scripts for multiple cloud destinations. Instead of limiting our users to AWS, we could allow them to select between AWS and any other cloud providers we\u2019ve added support for (e.g. Azure). This was a lower priority for us, though, because AWS is by far the most popular cloud infrastructure provider."}),"\n",(0,o.jsx)(t.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,o.jsx)(t.p,{children:"Unilogs provides a unique middle ground between managed services and DIY solutions:"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Easy deployment"}),": Single-command setup with minimal prerequisites"]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Production-ready"}),": Handles TBs of logs per day with automatic scaling"]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Reliable"}),": Kafka-based architecture prevents data loss during bursts"]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Self-hosted"}),": Maintain full control and ownership of your log data"]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Cost-effective"}),": Pay only for the AWS infrastructure you use"]}),"\n"]}),"\n",(0,o.jsx)(t.p,{children:"By making sophisticated log observability accessible to teams without specialized expertise, Unilogs fills an important gap in the observability landscape. Our opinionated approach abstracts away the complexity of configuration and deployment while maintaining the scalability and reliability needed for production workloads."}),"\n",(0,o.jsx)(t.p,{children:"The Unilogs architecture demonstrates how careful design choices can yield a system that is both powerful and easy to use. From the Vector shippers collecting logs at the edge, through the Kafka queue handling bursts, to Loki efficiently storing and querying log data, and finally Grafana providing rich visualizations\u2013each component plays a crucial role in delivering a complete log observability solution."}),"\n",(0,o.jsx)(t.p,{children:"As we look to the future, we're excited about expanding Unilogs' capabilities while maintaining our core commitment to simplicity and reliability. Whether adding support for additional telemetry types, enhancing configuration options, or exploring multi-cloud deployments, our goal remains the same: to provide teams with the observability tools they need to understand and improve their distributed applications."})]})}function d(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,t,i)=>{i.d(t,{R:()=>s,x:()=>r});var o=i(6540);const a={},n=o.createContext(a);function s(e){const t=o.useContext(n);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),o.createElement(n.Provider,{value:t},e.children)}}}]);